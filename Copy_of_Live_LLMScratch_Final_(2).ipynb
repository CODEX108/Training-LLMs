{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfTQC5-R9KkG"
      },
      "source": [
        "# BUILD A LARGE LANGUAGE MODEL FROM SCRATCH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1rpOiY49KkH"
      },
      "source": [
        "### STEP 1: LOADING THE DATASET\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsPpTAotu7gK",
        "outputId": "fa5e5158-3de6-482e-8dbd-aeac78c6494d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File 'autob-yogi.txt' loaded with 182706 characters.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "file_path = \"autob-yogi.txt\"\n",
        "\n",
        "# Check if the file exists in the current directory (i.e., Colab session)\n",
        "if os.path.exists(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        text_data = file.read()\n",
        "    print(f\"File '{file_path}' loaded with {len(text_data)} characters.\")\n",
        "else:\n",
        "    print(f\"File '{file_path}' not found. Please upload it to the Colab environment first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0g39tUJ9KkI"
      },
      "source": [
        "### STEP 2: IMPLEMENTING THE TOKENIZER\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAStBoVS9KkI"
      },
      "outputs": [],
      "source": [
        "!pip3 install tiktoken > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvvZUNnH9KkI",
        "outputId": "7f835cc9-31ad-44a8-8bf2-826132b65278"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tiktoken version: 0.9.0\n"
          ]
        }
      ],
      "source": [
        "import importlib\n",
        "import tiktoken\n",
        "\n",
        "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zPCCnGu9KkJ"
      },
      "outputs": [],
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BLKO6Xh9KkJ",
        "outputId": "14db59e6-2de4-4c49-e3b9-8bf2fdd1f78c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
          ]
        }
      ],
      "source": [
        "text = (\n",
        "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
        "     \"of someunknownPlace.\"\n",
        ")\n",
        "\n",
        "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "print(integers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdUX0CEb9KkJ",
        "outputId": "5d7bf740-a306-4129-8d65-3bc089c04614"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[33901, 86, 343, 86, 220, 959]\n",
            "Akwirw ier\n"
          ]
        }
      ],
      "source": [
        "integers = tokenizer.encode(\"Akwirw ier\")\n",
        "print(integers)\n",
        "\n",
        "strings = tokenizer.decode(integers)\n",
        "print(strings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayGpRaun91ku",
        "outputId": "070db967-bc31-463d-8bbe-bbe233e56eab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Characters: 182706\n",
            "Tokens: 48631\n"
          ]
        }
      ],
      "source": [
        "total_characters = len(text_data)\n",
        "total_tokens = len(tokenizer.encode(text_data))\n",
        "\n",
        "print(\"Characters:\", total_characters)\n",
        "print(\"Tokens:\", total_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVpgQJxO9KkJ"
      },
      "source": [
        "### STEP 3: CREATING INPUT-TARGET PAIRS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-UdNDW59KkJ",
        "outputId": "68baf836-4a42-40a5-872b-8835cb6ce36a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "48631\n"
          ]
        }
      ],
      "source": [
        "with open(\"autob-yogi.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "enc_text = tokenizer.encode(raw_text)\n",
        "print(len(enc_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvwzHMOM9KkJ"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FA3xjMcL9KkJ"
      },
      "outputs": [],
      "source": [
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0):\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8QvsSIs9KkJ",
        "outputId": "00508ad3-aa83-4b13-c9b4-ef61114f300a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.6.0+cu124\n",
            "[tensor([[39371,    46,  3483, 49656]]), tensor([[   46,  3483, 49656, 31300]])]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
        ")\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7PFNk_Q9KkJ"
      },
      "source": [
        "### STEP 4: CREATING TOKEN EMBEDDINGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r59Haayq9KkJ"
      },
      "outputs": [],
      "source": [
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ec72YAl9KkJ"
      },
      "outputs": [],
      "source": [
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=8, max_length=max_length,\n",
        "    stride=max_length, shuffle=False\n",
        ")\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYpQsV1c9KkK",
        "outputId": "b883da02-93f0-4b98-8644-0250200d4dda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token IDs:\n",
            " tensor([[39371,    46,  3483, 49656],\n",
            "        [31300,    56,  3963,   317],\n",
            "        [  575,  7730,    40,   628],\n",
            "        [  198,  3886, 25139,    71],\n",
            "        [  504,    64,   575,  9632],\n",
            "        [ 5282,   198,   198,    54],\n",
            "        [10554,   317, 22814, 49836],\n",
            "        [11050,   198,    54,    13]])\n",
            "\n",
            "Inputs shape:\n",
            " torch.Size([8, 4])\n"
          ]
        }
      ],
      "source": [
        "print(\"Token IDs:\\n\", inputs)\n",
        "print(\"\\nInputs shape:\\n\", inputs.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZrkE2Qm9KkK",
        "outputId": "60c2c33e-dcd3-4c5a-ae0a-23c5496256a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ],
      "source": [
        "token_embeddings = token_embedding_layer(inputs)\n",
        "print(token_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8smgXdd9KkK"
      },
      "source": [
        "### STEP 5: CREATING POSITIONAL EMBEDDINGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RwqymHS9KkK"
      },
      "outputs": [],
      "source": [
        "context_length = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjcgOLZB9KkK",
        "outputId": "007a6cef-c589-49b7-c6fe-900cb1583eef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 256])\n"
          ]
        }
      ],
      "source": [
        "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
        "print(pos_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSaYi0av9KkK"
      },
      "source": [
        "### STEP 6: CREATING INPUT EMBEDDINGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSeEO2BR9KkK",
        "outputId": "aeec7586-7fd4-496f-db9f-807c904cdc69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ],
      "source": [
        "input_embeddings = token_embeddings + pos_embeddings\n",
        "print(input_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cddJBU79KkK"
      },
      "source": [
        "### STEP 7: IMPLEMENTING MULTI-HEAD ATTENTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOK38nTO9KkK"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \\\n",
        "            \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length),\n",
        "                       diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPNlDRLg9KkK",
        "outputId": "6ed5b403-8eb6-4eaa-aa9b-03f4cc4dc4b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 3, 6])\n",
            "tensor([[[ 0.1569, -0.0873,  0.0210,  0.0215, -0.3243, -0.2518],\n",
            "         [ 0.1117, -0.0547,  0.0406, -0.0213, -0.3251, -0.2993],\n",
            "         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]],\n",
            "\n",
            "        [[ 0.1569, -0.0873,  0.0210,  0.0215, -0.3243, -0.2518],\n",
            "         [ 0.1117, -0.0547,  0.0406, -0.0213, -0.3251, -0.2993],\n",
            "         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 3, 6])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "# Define the tensor with 3 rows and 6 columns\n",
        "inputs = torch.tensor(\n",
        "    [[0.43, 0.15, 0.89, 0.55, 0.87, 0.66],  # Row 1\n",
        "     [0.57, 0.85, 0.64, 0.22, 0.58, 0.33],  # Row 2\n",
        "     [0.77, 0.25, 0.10, 0.05, 0.80, 0.55]]  # Row 3\n",
        ")\n",
        "\n",
        "batch = torch.stack((inputs, inputs), dim=0)\n",
        "print(batch.shape)\n",
        "\n",
        "batch_size, context_length, d_in = batch.shape\n",
        "d_out = 6\n",
        "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
        "context_vecs = mha(batch)\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9nmF0vx9KkK"
      },
      "source": [
        "### STEP 8: IMPLEMENTING A GPT MODEL FROM SCRATCH TO GENERATE TEXT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0M8o78gL9KkK"
      },
      "outputs": [],
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,    # Vocabulary size\n",
        "    \"context_length\": 256, # Context length\n",
        "    \"emb_dim\": 768,         # Embedding dimension\n",
        "    \"n_heads\": 12,          # Number of attention heads\n",
        "    \"n_layers\": 3,         # Number of layers\n",
        "    \"drop_rate\": 0.1,       # Dropout rate\n",
        "    \"qkv_bias\": False       # Query-Key-Value bias\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biBIbfH59KkK"
      },
      "source": [
        "### STEP 9: THE BUILDING BLOCKS-LAYER NORMALIZATION, GELU AND FEED-FORWARD NEURAL NETWORK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43IkKn1t9KkK"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), ## Expansion\n",
        "            GELU(), ## Activation\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), ## Contraction\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cc7qPADt9KkK"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        # Shortcut connection for feed forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        # 2*4*768\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x\n",
        "        # 2*4*768"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nw0QKZOa9KkL"
      },
      "source": [
        "### STEP 10: ENTIRE GPT MODEL ARCHITECTURE IMPLEMENTATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMfj2tEz9KkL"
      },
      "outputs": [],
      "source": [
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKB4thW19KkL",
        "outputId": "88e5f58f-73a3-4313-e707-8a7b44261a64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input batch:\n",
            " tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n",
            "\n",
            "Output shape: torch.Size([2, 4, 50257])\n",
            "tensor([[[-0.0171,  0.3346,  0.0099,  ..., -0.5861, -0.1948, -0.1229],\n",
            "         [ 0.1867, -0.1159, -0.0984,  ..., -0.6584, -1.3360, -0.0626],\n",
            "         [-0.0687,  0.2076,  0.9118,  ...,  1.1204,  0.7608,  0.3429],\n",
            "         [ 0.1777,  0.3090,  0.0644,  ..., -0.1662,  0.6492, -0.7036]],\n",
            "\n",
            "        [[ 0.0607,  0.3491, -0.1514,  ..., -0.5657,  0.1215, -0.1264],\n",
            "         [ 0.1742, -0.4437,  0.2715,  ..., -0.2964,  0.3105, -0.7598],\n",
            "         [ 0.0436, -0.4973,  0.1537,  ...,  0.6663,  0.4859, -0.3535],\n",
            "         [ 0.2986, -0.1419,  0.8008,  ..., -0.0188, -0.2154, -1.3115]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "batch = []\n",
        "txt1 = \"Every effort moves you\"\n",
        "txt2 = \"Every day holds a\"\n",
        "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
        "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
        "batch = torch.stack(batch, dim=0)\n",
        "\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "out = model(batch)\n",
        "print(\"Input batch:\\n\", batch)\n",
        "print(\"\\nOutput shape:\", out.shape)\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQOyQ3tl9KkL",
        "outputId": "bc6a72e9-4841-417b-e014-7c1321c2dd3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of parameters: 84,478,464\n"
          ]
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters: {total_params:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2obfC_B9KkL",
        "outputId": "0516334d-560d-4da3-9199-2a45bbf7f0ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token embedding layer shape: torch.Size([50257, 768])\n",
            "Output layer shape: torch.Size([50257, 768])\n"
          ]
        }
      ],
      "source": [
        "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
        "print(\"Output layer shape:\", model.out_head.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITBHaII99KkL",
        "outputId": "b1f9344d-222e-4a96-a2aa-4a34d2ed7e9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total size of the model: 322.26 MB\n"
          ]
        }
      ],
      "source": [
        "total_size_bytes = total_params * 4 #A\n",
        "total_size_mb = total_size_bytes / (1024 * 1024) #B\n",
        "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cl9AKus9KkM"
      },
      "source": [
        "### STEP 11: GENERATING TEXT FROM OUTPUT TOKENS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXKyFLBM9KkM"
      },
      "outputs": [],
      "source": [
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    # idx is (batch, n_tokens) array of indices in the current context\n",
        "\n",
        "    ###Input batch:\n",
        " ###tensor([[6109, 3626, 6100,  345],\n",
        "        ##[6109, 1110, 6622,  257]])\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "        # Crop current context if it exceeds the supported context size\n",
        "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
        "        # then only the last 5 tokens are used as context\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "\n",
        "        # Get the predictions\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond) ### batch, n_tokens, vocab_size\n",
        "\n",
        "        # Focus only on the last time step\n",
        "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # Apply softmax to get probabilities\n",
        "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
        "\n",
        "        # Get the idx of the vocab entry with the highest probability value\n",
        "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
        "\n",
        "        # Append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
        "\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7FRZD3I9KkM",
        "outputId": "2d0f1960-4e71-438b-86cc-2fcea3b227ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output text:\n",
            " He said we came here者 donated German affirmsystemofficeDX サーティ vampires township\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0) # remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())\n",
        "\n",
        "start_context = \"He said we came here\"\n",
        "\n",
        "\n",
        "\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(start_context, tokenizer),\n",
        "    max_new_tokens=10,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3Ci5VMs9KkM"
      },
      "source": [
        "### STEP 12: CREATING TRAINING, TESTING AND VALIDATION DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-3rTwWE9KkM"
      },
      "outputs": [],
      "source": [
        "# Train/validation ratio\n",
        "train_ratio = 0.90\n",
        "split_idx = int(train_ratio * len(text_data))\n",
        "train_data = text_data[:split_idx]\n",
        "val_data = text_data[split_idx:]\n",
        "\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = create_dataloader_v1(\n",
        "    train_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=True,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_loader = create_dataloader_v1(\n",
        "    val_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=False,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSrx9xtq9KkM"
      },
      "source": [
        "### STEP 13: DEFINING THE CROSS ENTROPY LOSS FUNCTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdweT7JQ9KkM"
      },
      "outputs": [],
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "    return loss\n",
        "\n",
        "\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # Reduce the number of batches to match the total number of batches in the data loader\n",
        "        # if num_batches exceeds the number of batches in the data loader\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uvcpcdt79KkM",
        "outputId": "d2023cc0-03ed-4e1c-efba-fb7907b12ff8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 10.975116034115063\n",
            "Validation loss: 10.965743923187256\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Note:\n",
        "# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n",
        "# which is approximately 2x faster than on an Apple CPU (as measured on an M3 MacBook Air).\n",
        "# However, the resulting loss values may be slightly different.\n",
        "\n",
        "#if torch.cuda.is_available():\n",
        "#    device = torch.device(\"cuda\")\n",
        "#elif torch.backends.mps.is_available():\n",
        "#    device = torch.device(\"mps\")\n",
        "#else:\n",
        "#    device = torch.device(\"cpu\")\n",
        "#\n",
        "# print(f\"Using {device} device.\")\n",
        "\n",
        "\n",
        "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
        "\n",
        "\n",
        "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
        "\n",
        "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
        "    train_loss = calc_loss_loader(train_loader, model, device)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device)\n",
        "\n",
        "print(\"Training loss:\", train_loss)\n",
        "print(\"Validation loss:\", val_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtSU2mGr-myP",
        "outputId": "a4836677-713d-406d-9e93-7f4bc9ec17b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIKwRtJw9KkM"
      },
      "source": [
        "### STEP 14: TRAINING LOOP FOR THE LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7C-_OU5H9KkM"
      },
      "outputs": [],
      "source": [
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context, tokenizer):\n",
        "    # Initialize lists to track losses and tokens seen\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward() # Calculate loss gradients\n",
        "            optimizer.step() # Update model weights using loss gradients\n",
        "            tokens_seen += input_batch.numel() # Returns the total number of elements (or tokens) in the input_batch.\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional evaluation step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Print a sample text after each epoch\n",
        "        generate_and_print_sample(\n",
        "            model, tokenizer, device, start_context\n",
        "        )\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMkUVPrN9KkM"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuHrwhUK9KkN"
      },
      "outputs": [],
      "source": [
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate_text_simple(\n",
        "            model=model, idx=encoded,\n",
        "            max_new_tokens=50, context_size=context_size\n",
        "        )\n",
        "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
        "    model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5DsoYxn9KkN",
        "outputId": "222e702b-65d8-4969-8abf-85dc762c6ae0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep 1 (Step 000000): Train loss 10.527, Val loss 10.564\n",
            "Ep 1 (Step 000005): Train loss 8.634, Val loss 8.682\n",
            "Ep 1 (Step 000010): Train loss 7.828, Val loss 7.843\n",
            "Ep 1 (Step 000015): Train loss 7.274, Val loss 7.521\n",
            "Ep 1 (Step 000020): Train loss 7.067, Val loss 7.428\n",
            "Ep 1 (Step 000025): Train loss 6.250, Val loss 7.357\n",
            "Ep 1 (Step 000030): Train loss 6.734, Val loss 7.257\n",
            "Ep 1 (Step 000035): Train loss 6.690, Val loss 7.160\n",
            "Ep 1 (Step 000040): Train loss 6.668, Val loss 7.068\n",
            "Ep 1 (Step 000045): Train loss 6.971, Val loss 7.026\n",
            "Ep 1 (Step 000050): Train loss 6.919, Val loss 6.972\n",
            "Ep 1 (Step 000055): Train loss 6.580, Val loss 6.978\n",
            "Ep 1 (Step 000060): Train loss 6.531, Val loss 6.903\n",
            "Ep 1 (Step 000065): Train loss 5.886, Val loss 6.829\n",
            "Ep 1 (Step 000070): Train loss 5.519, Val loss 6.773\n",
            "Ep 1 (Step 000075): Train loss 6.410, Val loss 6.760\n",
            "Ep 1 (Step 000080): Train loss 5.726, Val loss 6.690\n",
            "He said we came here “The. “The “The “The.  “The of the Himal “The of the Himal “The “The of the Himal  ““\n",
            "Ep 2 (Step 000085): Train loss 6.033, Val loss 6.642\n",
            "Ep 2 (Step 000090): Train loss 5.590, Val loss 6.674\n",
            "Ep 2 (Step 000095): Train loss 5.764, Val loss 6.616\n",
            "Ep 2 (Step 000100): Train loss 5.309, Val loss 6.646\n",
            "Ep 2 (Step 000105): Train loss 5.476, Val loss 6.626\n",
            "Ep 2 (Step 000110): Train loss 6.632, Val loss 6.589\n",
            "Ep 2 (Step 000115): Train loss 6.271, Val loss 6.595\n",
            "Ep 2 (Step 000120): Train loss 5.515, Val loss 6.618\n",
            "Ep 2 (Step 000125): Train loss 5.110, Val loss 6.568\n",
            "Ep 2 (Step 000130): Train loss 5.514, Val loss 6.592\n",
            "Ep 2 (Step 000135): Train loss 5.360, Val loss 6.522\n",
            "Ep 2 (Step 000140): Train loss 4.777, Val loss 6.524\n",
            "Ep 2 (Step 000145): Train loss 5.167, Val loss 6.512\n",
            "Ep 2 (Step 000150): Train loss 4.575, Val loss 6.504\n",
            "Ep 2 (Step 000155): Train loss 4.842, Val loss 6.504\n",
            "Ep 2 (Step 000160): Train loss 5.703, Val loss 6.475\n",
            "Ep 2 (Step 000165): Train loss 5.192, Val loss 6.454\n",
            "He said we came here, I had                             ” I am was the            \n",
            "Ep 3 (Step 000170): Train loss 4.330, Val loss 6.420\n",
            "Ep 3 (Step 000175): Train loss 4.339, Val loss 6.448\n",
            "Ep 3 (Step 000180): Train loss 4.506, Val loss 6.450\n",
            "Ep 3 (Step 000185): Train loss 4.029, Val loss 6.452\n",
            "Ep 3 (Step 000190): Train loss 4.865, Val loss 6.474\n",
            "Ep 3 (Step 000195): Train loss 4.058, Val loss 6.458\n",
            "Ep 3 (Step 000200): Train loss 4.065, Val loss 6.496\n",
            "Ep 3 (Step 000205): Train loss 4.180, Val loss 6.471\n",
            "Ep 3 (Step 000210): Train loss 4.348, Val loss 6.490\n",
            "Ep 3 (Step 000215): Train loss 4.873, Val loss 6.531\n",
            "Ep 3 (Step 000220): Train loss 4.066, Val loss 6.517\n",
            "Ep 3 (Step 000225): Train loss 4.844, Val loss 6.493\n",
            "Ep 3 (Step 000230): Train loss 4.375, Val loss 6.484\n",
            "Ep 3 (Step 000235): Train loss 4.464, Val loss 6.474\n",
            "Ep 3 (Step 000240): Train loss 4.001, Val loss 6.435\n",
            "Ep 3 (Step 000245): Train loss 3.616, Val loss 6.420\n",
            "Ep 3 (Step 000250): Train loss 3.494, Val loss 6.413\n",
            "He said we came here the   “I will not the    ““““““““““““““““““\n",
            "Ep 4 (Step 000255): Train loss 3.873, Val loss 6.421\n",
            "Ep 4 (Step 000260): Train loss 3.722, Val loss 6.437\n",
            "Ep 4 (Step 000265): Train loss 3.685, Val loss 6.476\n",
            "Ep 4 (Step 000270): Train loss 3.849, Val loss 6.504\n",
            "Ep 4 (Step 000275): Train loss 3.412, Val loss 6.525\n",
            "Ep 4 (Step 000280): Train loss 3.286, Val loss 6.483\n",
            "Ep 4 (Step 000285): Train loss 3.939, Val loss 6.503\n",
            "Ep 4 (Step 000290): Train loss 3.398, Val loss 6.509\n",
            "Ep 4 (Step 000295): Train loss 4.046, Val loss 6.521\n",
            "Ep 4 (Step 000300): Train loss 3.204, Val loss 6.525\n",
            "Ep 4 (Step 000305): Train loss 3.131, Val loss 6.555\n",
            "Ep 4 (Step 000310): Train loss 3.293, Val loss 6.502\n",
            "Ep 4 (Step 000315): Train loss 3.090, Val loss 6.538\n",
            "Ep 4 (Step 000320): Train loss 2.342, Val loss 6.497\n",
            "Ep 4 (Step 000325): Train loss 3.435, Val loss 6.488\n",
            "Ep 4 (Step 000330): Train loss 3.455, Val loss 6.539\n",
            "Ep 4 (Step 000335): Train loss 2.649, Val loss 6.548\n",
            "He said we came here, he had been of the the guru”  The swami Pranabananda, I felt doub. The ’s problem, I realized   ’s. I have known ’s.\n",
            "Ep 5 (Step 000340): Train loss 2.797, Val loss 6.553\n",
            "Ep 5 (Step 000345): Train loss 2.762, Val loss 6.517\n",
            "Ep 5 (Step 000350): Train loss 2.807, Val loss 6.557\n",
            "Ep 5 (Step 000355): Train loss 2.100, Val loss 6.580\n",
            "Ep 5 (Step 000360): Train loss 2.223, Val loss 6.652\n",
            "Ep 5 (Step 000365): Train loss 2.373, Val loss 6.656\n",
            "Ep 5 (Step 000370): Train loss 2.228, Val loss 6.656\n",
            "Ep 5 (Step 000375): Train loss 2.610, Val loss 6.665\n",
            "Ep 5 (Step 000380): Train loss 2.665, Val loss 6.665\n",
            "Ep 5 (Step 000385): Train loss 2.754, Val loss 6.686\n",
            "Ep 5 (Step 000390): Train loss 1.824, Val loss 6.658\n",
            "Ep 5 (Step 000395): Train loss 2.255, Val loss 6.643\n",
            "Ep 5 (Step 000400): Train loss 1.821, Val loss 6.690\n",
            "Ep 5 (Step 000405): Train loss 2.107, Val loss 6.716\n",
            "Ep 5 (Step 000410): Train loss 2.316, Val loss 6.724\n",
            "Ep 5 (Step 000415): Train loss 1.569, Val loss 6.766\n",
            "Ep 5 (Step 000420): Train loss 1.930, Val loss 6.776\n",
            "He said we came here, he departed with a your sister Roma, I had an ominous to the common fate of Amar I felt doubly. “You have no more, and shattering.”  ”  ““\n",
            "Ep 6 (Step 000425): Train loss 1.612, Val loss 6.733\n",
            "Ep 6 (Step 000430): Train loss 1.719, Val loss 6.788\n",
            "Ep 6 (Step 000435): Train loss 1.465, Val loss 6.812\n",
            "Ep 6 (Step 000440): Train loss 1.902, Val loss 6.833\n",
            "Ep 6 (Step 000445): Train loss 1.717, Val loss 6.902\n",
            "Ep 6 (Step 000450): Train loss 0.961, Val loss 6.893\n",
            "Ep 6 (Step 000455): Train loss 2.017, Val loss 6.914\n",
            "Ep 6 (Step 000460): Train loss 1.347, Val loss 6.921\n",
            "Ep 6 (Step 000465): Train loss 2.035, Val loss 6.954\n",
            "Ep 6 (Step 000470): Train loss 1.543, Val loss 6.947\n",
            "Ep 6 (Step 000475): Train loss 1.759, Val loss 6.962\n",
            "Ep 6 (Step 000480): Train loss 1.425, Val loss 6.933\n",
            "Ep 6 (Step 000485): Train loss 1.301, Val loss 6.947\n",
            "Ep 6 (Step 000490): Train loss 1.140, Val loss 6.974\n",
            "Ep 6 (Step 000495): Train loss 1.506, Val loss 7.018\n",
            "Ep 6 (Step 000500): Train loss 0.862, Val loss 7.031\n",
            "Ep 6 (Step 000505): Train loss 1.066, Val loss 7.094\n",
            "He said we came here for a life of unbroken communion considered his expand. My own purpose is distance.’s sole path of his own  ’s speech was Divinity  “Think you not yet told me how I\n",
            "Ep 7 (Step 000510): Train loss 1.145, Val loss 7.110\n",
            "Ep 7 (Step 000515): Train loss 0.841, Val loss 7.167\n",
            "Ep 7 (Step 000520): Train loss 0.946, Val loss 7.176\n",
            "Ep 7 (Step 000525): Train loss 0.561, Val loss 7.179\n",
            "Ep 7 (Step 000530): Train loss 0.369, Val loss 7.211\n",
            "Ep 7 (Step 000535): Train loss 0.710, Val loss 7.240\n",
            "Ep 7 (Step 000540): Train loss 0.826, Val loss 7.256\n",
            "Ep 7 (Step 000545): Train loss 0.364, Val loss 7.324\n",
            "Ep 7 (Step 000550): Train loss 0.632, Val loss 7.289\n",
            "Ep 7 (Step 000555): Train loss 0.721, Val loss 7.292\n",
            "Ep 7 (Step 000560): Train loss 1.364, Val loss 7.287\n",
            "Ep 7 (Step 000565): Train loss 1.269, Val loss 7.302\n",
            "Ep 7 (Step 000570): Train loss 0.379, Val loss 7.296\n",
            "Ep 7 (Step 000575): Train loss 0.564, Val loss 7.347\n",
            "Ep 7 (Step 000580): Train loss 0.406, Val loss 7.314\n",
            "Ep 7 (Step 000585): Train loss 0.690, Val loss 7.345\n",
            "Ep 7 (Step 000590): Train loss 0.546, Val loss 7.334\n",
            "He said we came here today; I convinced him austere face was noticeably softened. I sat rigidly motionless on the hoard of my dwelling place. He was also be   Soon we entered a university hall where a lecture was seated on a visit.\n",
            "Ep 8 (Step 000595): Train loss 0.650, Val loss 7.407\n",
            "Ep 8 (Step 000600): Train loss 0.544, Val loss 7.449\n",
            "Ep 8 (Step 000605): Train loss 0.574, Val loss 7.503\n",
            "Ep 8 (Step 000610): Train loss 0.521, Val loss 7.513\n",
            "Ep 8 (Step 000615): Train loss 0.717, Val loss 7.586\n",
            "Ep 8 (Step 000620): Train loss 0.743, Val loss 7.515\n",
            "Ep 8 (Step 000625): Train loss 0.682, Val loss 7.477\n",
            "Ep 8 (Step 000630): Train loss 0.676, Val loss 7.575\n",
            "Ep 8 (Step 000635): Train loss 0.231, Val loss 7.546\n",
            "Ep 8 (Step 000640): Train loss 0.332, Val loss 7.508\n",
            "Ep 8 (Step 000645): Train loss 0.510, Val loss 7.543\n",
            "Ep 8 (Step 000650): Train loss 0.760, Val loss 7.636\n",
            "Ep 8 (Step 000655): Train loss 0.422, Val loss 7.573\n",
            "Ep 8 (Step 000660): Train loss 0.320, Val loss 7.572\n",
            "Ep 8 (Step 000665): Train loss 0.355, Val loss 7.566\n",
            "Ep 8 (Step 000670): Train loss 0.389, Val loss 7.640\n",
            "Ep 8 (Step 000675): Train loss 0.288, Val loss 7.670\n",
            "He said we came here today. This interrogatory period,  ILLUSTRATIONS     Frontispiece   Map of the more hungrily he sniffed   PARAMHANSA YOGANAN   Swami Pranabananda,\n",
            "Ep 9 (Step 000680): Train loss 0.465, Val loss 7.701\n",
            "Ep 9 (Step 000685): Train loss 0.446, Val loss 7.767\n",
            "Ep 9 (Step 000690): Train loss 0.219, Val loss 7.740\n",
            "Ep 9 (Step 000695): Train loss 0.416, Val loss 7.766\n",
            "Ep 9 (Step 000700): Train loss 0.321, Val loss 7.854\n",
            "Ep 9 (Step 000705): Train loss 0.355, Val loss 7.815\n",
            "Ep 9 (Step 000710): Train loss 0.410, Val loss 7.822\n",
            "Ep 9 (Step 000715): Train loss 0.308, Val loss 7.842\n",
            "Ep 9 (Step 000720): Train loss 0.469, Val loss 7.894\n",
            "Ep 9 (Step 000725): Train loss 0.274, Val loss 7.920\n",
            "Ep 9 (Step 000730): Train loss 0.294, Val loss 7.881\n",
            "Ep 9 (Step 000735): Train loss 0.301, Val loss 7.885\n",
            "Ep 9 (Step 000740): Train loss 0.311, Val loss 7.844\n",
            "Ep 9 (Step 000745): Train loss 0.212, Val loss 7.905\n",
            "Ep 9 (Step 000750): Train loss 0.246, Val loss 7.955\n",
            "Ep 9 (Step 000755): Train loss 0.205, Val loss 7.952\n",
            "Ep 9 (Step 000760): Train loss 0.149, Val loss 7.981\n",
            "He said we came here today; I convinced him blessing. The next evening, as I sat with folded hands in meditation, a silver amulet materialized between my palms, a chair of simpler design.”    The saint, ignoring\n",
            "Ep 10 (Step 000765): Train loss 0.159, Val loss 7.958\n",
            "Ep 10 (Step 000770): Train loss 0.162, Val loss 7.988\n",
            "Ep 10 (Step 000775): Train loss 0.143, Val loss 7.997\n",
            "Ep 10 (Step 000780): Train loss 0.081, Val loss 8.017\n",
            "Ep 10 (Step 000785): Train loss 0.221, Val loss 8.011\n",
            "Ep 10 (Step 000790): Train loss 0.232, Val loss 8.077\n",
            "Ep 10 (Step 000795): Train loss 0.175, Val loss 8.023\n",
            "Ep 10 (Step 000800): Train loss 0.111, Val loss 7.958\n",
            "Ep 10 (Step 000805): Train loss 0.232, Val loss 7.970\n",
            "Ep 10 (Step 000810): Train loss 0.122, Val loss 8.018\n",
            "Ep 10 (Step 000815): Train loss 0.110, Val loss 8.066\n",
            "Ep 10 (Step 000820): Train loss 0.141, Val loss 8.042\n",
            "Ep 10 (Step 000825): Train loss 0.224, Val loss 8.033\n",
            "Ep 10 (Step 000830): Train loss 0.141, Val loss 8.046\n",
            "Ep 10 (Step 000835): Train loss 0.179, Val loss 8.089\n",
            "Ep 10 (Step 000840): Train loss 0.163, Val loss 8.091\n",
            "Ep 10 (Step 000845): Train loss 0.166, Val loss 8.076\n",
            "He said we came here today. This interrogatory period, like the sepulchral haunts, inspires a well-known terror. My mind was nevertheless at peace. Braving the ghouls, I was exhuming a knowledge not found in lecture \n",
            "Training completed in 1.37 minutes.\n"
          ]
        }
      ],
      "source": [
        "#!pip uninstall sympy -y\n",
        "#!pip install sympy==1.12\n",
        "\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1) #A\n",
        "\n",
        "num_epochs =10\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=5, eval_iter=1,\n",
        "    start_context=\"He said we came here\", tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQT61N7UQwxj"
      },
      "source": [
        "As we can see, based on the results printed during the training, the training loss improve drastically, starting with a value of 9.558 and converging to 0.762.\n",
        "\n",
        "The language skills of\n",
        "the model have improved quite a lot. In the beginning, the model is only able to append\n",
        "commas to the start context (\"Every effort moves you,,,,,,,,,,,,\") or repeat the\n",
        "word \"and\". At the end of the training, it can generate grammatically correct text.\n",
        "\n",
        "\n",
        "Similar to the training set loss, we can see that the validation loss starts high (9.856)\n",
        "and decreases during the training. However, it never becomes as small as the training set\n",
        "loss and remains at 6.372 after the 10th epoch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxntuhe-9KkN"
      },
      "source": [
        "### STEP 15: PLOTTING THE LOSSES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "M-S3REVg9KkN",
        "outputId": "b52412bf-aa65-4b32-caaa-3a7230fa78ae"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "    # Plot training and validation loss against epochs\n",
        "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
        "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend(loc=\"upper right\")\n",
        "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
        "\n",
        "    # Create a second x-axis for tokens seen\n",
        "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
        "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
        "    ax2.set_xlabel(\"Tokens seen\")\n",
        "\n",
        "    fig.tight_layout()  # Adjust layout to make room\n",
        "    plt.savefig(\"loss-plot.png\")\n",
        "    plt.show()\n",
        "\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y13_PzVf9KkO",
        "outputId": "cc6f242a-f228-4324-ed6d-2285e863a61a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output text:\n",
            " one expects that I am going to traverse the great day, little test\n",
            "of a few thousand\n",
            "times; yet it.  So may\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move model to device\n",
        "model = model.to(device)\n",
        "\n",
        "# Prepare token_ids\n",
        "token_ids = text_to_token_ids(\"one expects that\", tokenizer)\n",
        "token_ids = token_ids.to(device)  # Move token_ids to the correct device\n",
        "\n",
        "# Generate text\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=token_ids,  # Ensure token_ids are on the correct device\n",
        "    max_new_tokens=25,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "# Convert token_ids back to text\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "em8o81yY9KkO"
      },
      "source": [
        "### STEP 16: IMPLEMENTING TEMPERATURE SCALING AND TOP-K SAMPLING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEjM-WZe9KkO"
      },
      "outputs": [],
      "source": [
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "\n",
        "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # New: Filter logits with top_k sampling\n",
        "        if top_k is not None:\n",
        "            # Keep only top_k values\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
        "\n",
        "        # New: Apply temperature scaling\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
        "\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
        "\n",
        "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "\n",
        "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
        "            break\n",
        "\n",
        "        # Same as before: append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
        "\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7tvi7eU9KkO",
        "outputId": "e28cb223-fb46-4947-943a-0763f9e2cbf1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output text:\n",
            " Every effort moves you can’s divine best final healing words\n",
            "Company in creatures imagine!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# Define the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move model to the device\n",
        "model = model.to(device)\n",
        "\n",
        "# Prepare token_ids\n",
        "token_ids = text_to_token_ids(\"Every effort moves you\", tokenizer)\n",
        "token_ids = token_ids.to(device)  # Move token_ids to the correct device\n",
        "\n",
        "# Generate text with specific parameters\n",
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=token_ids,  # Ensure token_ids are on the correct device\n",
        "    max_new_tokens=15,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
        "    top_k=25,\n",
        "    temperature=1.4\n",
        ")\n",
        "\n",
        "# Convert token_ids back to text and print\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72KIVEji9KkO"
      },
      "source": [
        "### STEP 17: SAVING THE MODEL PARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiGnuVoo9KkO"
      },
      "outputs": [],
      "source": [
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "torch.save(model.state_dict(), \"model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_LvD-R_9KkO",
        "outputId": "17b497b7-7297-4c44-a5be-be5850d459df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 768)\n",
              "  (pos_emb): Embedding(256, 768)\n",
              "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 148,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.load_state_dict(torch.load(\"model.pth\"))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20Dm2K5U9KkO"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01, weight_decay=0.1)\n",
        "\n",
        "torch.save({\n",
        "    \"model_state_dict\": model.state_dict(),\n",
        "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "    },\n",
        "    \"model_and_optimizer.pth\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNZpPSM09KkO"
      },
      "outputs": [],
      "source": [
        "checkpoint = torch.load(\"model_and_optimizer.pth\")\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
        "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "model.train();"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}